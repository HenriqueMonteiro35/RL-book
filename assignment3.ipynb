{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2024) - Assignment 3\n",
    "\n",
    "**Due: Jan 29 @ 11:59pm Pacific Time on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- **Please solve questions 1 and 2, and choose one of questions 3 or 4.**\n",
    "- Empty code blocks are for your use. Feel free to create more under each section as needed.\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/HenriqueMonteiro35/RL-book/blob/master/assignment3.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Arantxa Ramos del Valle - aramosv\n",
    "- Henrique Bittencourt Netto Monteiro - hbnm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**Analytic Optimal Actions and Cost.** \n",
    "Consider a continuous-states, continuous-actions, discrete-time, non-terminating MDP with state space as $\\mathbb{R}$ and action space as $\\mathbb{R}$. When in state $s\\in \\mathbb{R}$, upon taking action $a\\in \\mathbb{R}$, one transitions to next state $s' \\in \\mathbb{R}$ according to a normal distribution $s' \\sim \\mathcal{N}(s, \\sigma^2)$ for a fixed variance $\\sigma^2 \\in \\mathbb{R}^+$. The corresponding cost associated with this transition is $e^{as'}$, i.e., the cost depends on the action $a$ and the state $s'$ one transitions to. The problem is to minimize the infinite-horizon **Expected Discounted-Sum of Costs** (with discount factor $\\gamma < 1$). For this assignment, solve this problem just for the special case of $\\gamma = 0$ (i.e., the myopic case) using elementary calculus. Derive an analytic expression for the optimal action in any state and the corresponding optimal cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** since we are in the myopic case ($\\gamma = 0$), we just need to find a policy to minimize the expected cost of a single transaction. Since the cost is $e^{as'}$ and $s'$ is normally distributed, we can compute the expected cost $E[C]$ analytically,\n",
    "\n",
    "\\begin{align}\n",
    "E[C] &= E[e^{as'}] = \\int_{\\mathbb{R}} \\exp \\left( a s' \\right) \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp \\left( \\frac{-(s' - s)^2}{2 \\sigma^2} \\right) ds' \\nonumber \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\int_{\\mathbb{R}} \\exp \\left( \\frac{-s'^2 + 2 s s' - s^2 + 2 a \\sigma^2 s'}{2 \\sigma^2} \\right) ds' \\nonumber \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp \\left( \\frac{- s^2}{2 \\sigma^2} \\right)\n",
    "\\int_{\\mathbb{R}} \\exp \\left( \\frac{-s'^2 + 2 s' (s + a \\sigma^2)}{2 \\sigma^2} \\right) ds' \\nonumber \\\\\n",
    "&= \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}} \\exp \\left( \\frac{- s^2}{2 \\sigma^2} + \\frac{(s + a \\sigma^2)^2}{2 \\sigma^2} \\right)\n",
    "\\int_{\\mathbb{R}} \\exp \\left( \\frac{-s'^2 + 2 s' (s + a \\sigma^2) - (s + a \\sigma^2)^2}{2 \\sigma^2} \\right) ds' \\nonumber \\\\\n",
    "&= \\exp \\left( \\frac{2 a s \\sigma^2 + a^2 \\sigma^4}{2 \\sigma^2} \\right) \\left( \\frac{1}{\\sqrt{2 \\sigma^2 \\pi}}\n",
    "\\int_{\\mathbb{R}} \\exp \\left( \\frac{-(s' - (s + a \\sigma^2))^2}{2 \\sigma^2} \\right) ds' \\right) \\nonumber \\\\\n",
    "&= \\exp \\left( \\frac{2 a s + a^2 \\sigma^2}{2} \\right)\n",
    "\\end{align}\n",
    "\n",
    "in which in the fourth line we completed squares and in the fifth we used the fact that the expression in parenthesis was the integral of a Gaussian PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now note that the exponential is a strictly monotically increasing function, and the term $\\frac{2 a s + a^2 \\sigma^2}{2}$ is quadratic in $a$ with positive second derivative. Therefore, for any given $s$, minimizing the expected cost $E[C]$ with respect to the action $a$ is a convex problem with unique minimizer. We thus find this minimizer by differentiating with respect to $a$ and equating to zero,\n",
    "\n",
    "$$\n",
    "\\left( s + a \\sigma^2 \\right) \\exp \\left( \\frac{2 a s + a^2 \\sigma^2}{2} \\right) = 0 \\implies \\left( s + a \\sigma^2 \\right) = 0\n",
    "$$\n",
    "$$\n",
    "a = -\\frac{s}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "in which we used the fact that the exponential is never zero for any real number. We now compute the optimal cost $C^*$,\n",
    "\n",
    "$$\n",
    "C^* = \\exp \\left( -\\frac{s}{\\sigma^2} s + \\frac{s^2}{\\sigma^4} \\frac{\\sigma^2}{2} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C^* = \\exp \\left( - \\frac{s^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Therefore, in the myopic case, given any state $s$, the optimal action is $a^* = -\\frac{s}{\\sigma^2}$ and the optimal cost is $C^* = \\exp \\left( - \\frac{s^2}{2 \\sigma^2} \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**Manual Value Iteration.** \n",
    "Consider a simple MDP with $\\mathcal{S} = \\{s_1, s_2, s_3\\}, \\mathcal{T} =\\{s_3\\}, \\mathcal{A} = \\{a_1, a_2\\}$. The State Transition Probability function\n",
    "$$\\mathcal{P}: \\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$$\n",
    "is defined as:\n",
    "$$\\mathcal{P}(s_1, a_1, s_1) = 0.2, \\mathcal{P}(s_1, a_1, s_2) = 0.6, \\mathcal{P}(s_1, a_1, s_3) = 0.2$$\n",
    "$$\\mathcal{P}(s_1, a_2, s_1) = 0.1, \\mathcal{P}(s_1, a_2, s_2) = 0.2, \\mathcal{P}(s_1, a_2, s_3) = 0.7$$\n",
    "$$\\mathcal{P}(s_2, a_1, s_1) = 0.3, \\mathcal{P}(s_2, a_1, s_2) = 0.3, \\mathcal{P}(s_2, a_1, s_3) = 0.4$$\n",
    "$$\\mathcal{P}(s_2, a_2, s_1) = 0.5, \\mathcal{P}(s_2, a_2, s_2) = 0.3, \\mathcal{P}(s_2, a_2, s_3) = 0.2$$\n",
    "The Reward Function \n",
    "$$\\mathcal{R}: \\mathcal{N} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$$\n",
    "is defined as:\n",
    "$$\\mathcal{R}(s_1, a_1) = 8.0, \\mathcal{R}(s_1, a_2) = 10.0$$\n",
    "$$\\mathcal{R}(s_2, a_1) = 1.0, \\mathcal{R}(s_2, a_2) = -1.0$$\n",
    "Assume discount factor $\\gamma = 1$.\n",
    "\n",
    "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) simply the first two iterations of Value Iteration algorithm. \n",
    "\n",
    "- Initialize the Value Function for each state to be it's $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}( \\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
    "- Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. Hint: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "In every iteration, the value function state-action is calculated as follows:\n",
    "\n",
    "$${Q_{i+1}}(s,a)= \\mathcal{R}(s,a)+\\gamma\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot V_i(s')\\ for\\ all\\ s\\in\\mathcal{N}\\ for\\ all\\ a\\in\\mathcal{A}$$\n",
    "\n",
    "\n",
    "The value function is calculated:\n",
    "\n",
    "$${V_{i+1}}= B^*({V_{i}})(s)\\ for\\ all\\ s\\in\\mathcal{N}$$\n",
    "where,\n",
    "$$B^*({V_{i}})(s)=\\max_{a\\in\\mathcal{A}}\\{\\mathcal{R}(s,a)+\\gamma\\sum_{s'\\in\\mathcal{N}}\\mathcal{P}(s,a,s')\\cdot V(s')\\}\\ for\\ all\\ s\\in\\mathcal{N}$$\n",
    "\n",
    "Or what is the same:\n",
    "\n",
    "$${V_{i+1}}= \\max_{a\\in\\mathcal{A}}{Q_{i+1}}(s,a)$$\n",
    "\n",
    "Then the greedy policy is calculated, as follows:\n",
    "\n",
    "$${\\pi_{i+1}}= \\arg\\max_{a\\in\\mathcal{A}}{Q_{i+1}}(s,a)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We are initializing the value functions as: $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$\n",
    "\n",
    "In the first iteration ($k=1$):\n",
    "For the state $s_1$:\n",
    "\n",
    "- $q_1(s_1,a_1)=8.0+1\\cdot(0.2\\cdot10.0+0.6\\cdot1.0+0.2\\cdot0.0)=10.6$\n",
    "- $q_1(s_1,a_2)=10.0+1\\cdot(0.1\\cdot10.0+0.2\\cdot1.0+0.7\\cdot0.0)=11.2$\n",
    "\n",
    "\n",
    "$$v_1(s_1)=\\max\\{8.0+1\\cdot(0.2\\cdot10.0+0.6\\cdot1.0+0.2\\cdot0.0),10.0+1\\cdot(0.1\\cdot10.0+0.2\\cdot1.0+0.7\\cdot0.0)\\}=\\max\\{10.6,11.2\\}=11.2$$\n",
    "\n",
    "From this, the greedy policy is $\\pi_1(s_1)=a_2$\n",
    "\n",
    "\n",
    "For the state $s_2$:\n",
    "\n",
    "- $q_1(s_2,a_1)=1.0+1\\cdot(0.3\\cdot10.0+0.3\\cdot1.0+0.4\\cdot0.0)=4.3$\n",
    "- $q_1(s_2,a_2)=-1.0+1\\cdot(0.5\\cdot10.0+0.3\\cdot1.0+0.2\\cdot0.0)=4.3$\n",
    "\n",
    "\n",
    "$$v_1(s_2)=\\max\\{1.0+1\\cdot(0.3\\cdot10.0+0.3\\cdot1.0+0.4\\cdot0.0),-1.0+1\\cdot(0.5\\cdot10.0+0.3\\cdot1.0+0.2\\cdot0.0)\\}=\\max\\{4.3,4.3\\}=4.3$$\n",
    "\n",
    "From this, the greedy policy is $\\pi_1(s_2)=a_2$\n",
    "\n",
    "\n",
    "For the state $s_3$, $v_1(s_3)=0.0$ since it is a terminal state.\n",
    "\n",
    "\n",
    "For $k=2$:\n",
    "For the state $s_1$:\n",
    "\n",
    "- $q_2(s_1,a_1)=8.0+1\\cdot(0.2\\cdot11.2+0.6\\cdot4.3+0.2\\cdot0.0)=12.82$\n",
    "- $q_2(s_1,a_2)=10.0+1\\cdot(0.1\\cdot11.2+0.2\\cdot4.3+0.7\\cdot0.0)=11.98$\n",
    "\n",
    "\n",
    "$$v_2(s_1)=\\max\\{8.0+1\\cdot(0.2\\cdot11.2+0.6\\cdot4.3+0.2\\cdot0.0),10.0+1\\cdot(0.1\\cdot11.2+0.2\\cdot4.3+0.7\\cdot0.0)\\}=\\max\\{12.82,11.98\\}=12.82$$\n",
    "\n",
    "From this, the greedy policy is $\\pi_2(s_1)=a_1$\n",
    "\n",
    "\n",
    "For the state $s_2$:\n",
    "\n",
    "- $q_2(s_2,a_1)=1.0+1\\cdot(0.3\\cdot11.2+0.3\\cdot4.3+0.4\\cdot0.0)=5.65$\n",
    "- $q_2(s_2,a_2)=-1.0+1\\cdot(0.5\\cdot11.2+0.3\\cdot4.3+0.2\\cdot0.0)=5.89$\n",
    "\n",
    "\n",
    "$$v_2(s_2)=\\max\\{1.0+1\\cdot(0.3\\cdot11.2+0.3\\cdot4.3+0.4\\cdot0.0),-1.0+1\\cdot(0.5\\cdot11.2+0.3\\cdot4.3+0.2\\cdot0.0)\\}=\\max\\{5.65,5.89\\}=5.89$$\n",
    "\n",
    "From this, the greedy policy is $\\pi_2(s_2)=a_2$\n",
    "\n",
    "\n",
    "For the state $s_3$, $v_2(s_3)=0.0$ since it is a terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greedy policy $\\pi_k(s)$ for $k>2$ is the same policy for every state $s$ as the greedy policy $\\pi_2(s)$. Note that action $a_1$ at state $s_1$ has equal or higher probability to transition to each of $s_1$ and $s_2$ than $a_2$, and for state $s_2$, action $a_2$ has the same property compared to $a_1$. Given that and the fact that the value function is monotonically increasing at each iteration, these actions must be optimal as the gap in value between choosing them versus the alternative can only remain constant or widen at future iterations.\n",
    "\n",
    "Formally, for any state $k$, we have\n",
    "\n",
    "- $q_k(s_1, a_1) = \\mathcal{R}(s_1, a_1) + 0.2 v_{k-1}(s_1) + 0.6 v_{k-1}(s_1)$\n",
    "- $q_k(s_1, a_2) = \\mathcal{R}(s_1, a_2) + 0.1 v_{k-1}(s_1) + 0.2 v_{k-1}(s_1)$\n",
    "\n",
    "- $q_k(s_2, a_1) = \\mathcal{R}(s_2, a_1) + 0.3 v_{k-1}(s_1) + 0.3 v_{k-1}(s_1)$\n",
    "- $q_k(s_2, a_2) = \\mathcal{R}(s_2, a_2) + 0.5 v_{k-1}(s_1) + 0.3 v_{k-1}(s_1)$\n",
    "\n",
    "Then, the differences in value between taking each action is\n",
    "\n",
    "- $q_k(s_1, a_1) - q_k(s_1, a_2) = (\\mathcal{R}(s_1, a_1) - \\mathcal{R}(s_1, a_2)) + 0.1 v_{k-1}(s_1) + 0.4 v_{k-1}(s_2)$\n",
    "- $q_k(s_2, a_2) - q_k(s_2, a_1) = (\\mathcal{R}(s_2, a_2) - \\mathcal{R}(s_2, a_1)) + 0.2 v_{k-1}(s_1)$\n",
    "\n",
    "Finally,\n",
    "\n",
    "- $(q_k(s_1, a_1) - q_k(s_1, a_2)) - (q_2(s_1, a_1) - q_2(s_1, a_2)) = 0.1 (v_{k-1}(s_1) - v_{1}(s_1)) + 0.4 (v_{k-1}(s_2) - v_{1}(s_2))$\n",
    "- $(q_k(s_2, a_2) - q_k(s_2, a_1)) - (q_2(s_2, a_2) - q_2(s_2, a_1)) = 0.2 (v_{k-1}(s_1) - v_{1}(s_1))$\n",
    "\n",
    "Since the value function is monotonically increasing, the two above expressions are nonnegative for any $k > 2$, which means that for every possible iteration beyond $k = 2$, the values associated with $a_1$ at state $s_1$ and $a_2$ at state $s_2$ are higher than taking the alternative action,\n",
    "\n",
    "- $(q_k(s_1, a_1) - q_k(s_1, a_2)) \\geq (q_2(s_1, a_1) - q_2(s_1, a_2)) > 0 \\text{ } \\forall k > 2$\n",
    "- $(q_k(s_2, a_2) - q_k(s_2, a_1)) \\geq (q_2(s_2, a_2) - q_2(s_2, a_1)) > 0 \\text{ } \\forall k > 2$\n",
    "\n",
    " so the greedy policy will never change from $\\pi_2(\\cdot)$, and one does not need to go beyond iteration $k = 2$. Policy $\\pi_2(\\cdot)$ must be optimal for this MDP.\n",
    "\n",
    "\n",
    "\n",
    "<!-- Having a discount factor of 1 makes the future rewards as valuable as present rewards. The value functions are positive. Therefore, the action that makes less likely to reach the terminal state ($s_3$) is the greedy. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "**Job-Hopping and Wages-Utility-Maximization.** \n",
    "You are a worker who starts every day either employed or unemployed. If you start your day employed, you work on your job for the day (one of $n$ jobs, as elaborated later) and you get to earn the wage of the job for the day. However, at the end of the day, you could lose your job with probability $\\alpha \\in [0,1]$, in which case you start the next day unemployed. If at the end of the day, you do not lose your job (with probability $1-\\alpha$), then you will start the next day with the same job (and hence, the same daily wage). On the other hand, if you start your day unemployed, then you will be randomly offered one of $n$ jobs with daily wages $w_1, w_2, \\ldots w_n \\in \\mathbb{R}^+$ with respective job-offer probabilities $p_1, p_2, \\ldots p_n \\in [0,1]$ (with $\\sum_{i=1}^n p_i = 1$). You can choose to either accept or decline the offered job. If you accept the job-offer, your day progresses exactly like the **employed-day** described above (earning the day's job wage and possibly (with probability $\\alpha$) losing the job at the end of the day). However, if you decline the job-offer, you spend the day unemployed, receive the unemployment wage $w_0 \\in \\mathbb{R}^+$ for the day, and start the next day unemployed. The problem is to identify the optimal choice of accepting or rejecting any of the job-offers the worker receives, in a manner that maximizes the infinite-horizon **Expected Discounted-Sum of Wages Utility**. Assume the daily discount factor for wages (employed or unemployed) is $\\gamma \\in [0,1)$. Assume Wages Utility function to be $U(w) = \\log(w)$ for any wage amount $w \\in \\mathbb{R}^+$. So you are looking to maximize\n",
    "$$\\mathbb{E}[\\sum_{u=t}^\\infty \\gamma^{u-t} \\cdot \\log(w_{i_u})]$$\n",
    "at the start of a given day $t$ ($w_{i_u}$ is the wage earned on day $u$, $0\\leq i_u \\leq n$ for all $u\\geq t$).\n",
    "\n",
    "- Express with clear mathematical notation the state space, action space, transition function, reward function, and write the Bellman Optimality Equation customized for this MDP.\n",
    "- You can solve this Bellman Optimality Equation (hence, solve for the Optimal Value Function and the Optimal Policy) with a numerical iterative algorithm (essentially a Dynamic Programming algorithm customized to this problem). Write Python code for this numerical algorithm. Clearly define the inputs and outputs of your algorithm with their types (int, float, List, Mapping etc.). For this problem, don't use any of the MDP/DP code from the git repo, write this customized algorithm from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** we can define the state space as\n",
    "\n",
    "$$\n",
    "\\mathcal{S} = \\{s_1, s_2, \\dots, s_n, \\varsigma_1, \\varsigma_2, \\dots, \\varsigma_n \\}\n",
    "$$\n",
    "\n",
    "in which the states $s_i$ refer to starting the day employed at job $i$ while $\\varsigma_i$ refers to starting the day unemployed with an offer from job $i$.\n",
    "\n",
    "We can define the action space as\n",
    "\n",
    "$$\n",
    "\\mathcal{A} = \\{a_{w}, a_{r}\\}\n",
    "$$\n",
    "\n",
    "in which $a_w$ is to work for the given job (accepting the offer if unemployed) and $a_r$ is to reject the offer if unemployed. Clearly,\n",
    "\n",
    "$$\n",
    "\\mathcal{A} | s_i = \\{a_{w}\\} \\text{ and } \\mathcal{A} | \\varsigma_i = \\{a_{w}, a_r\\}\n",
    "$$\n",
    "\n",
    "since the option to reject and not work is only available if you start the day unemployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition function can be defined as\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(s_i | s_i, a_w) = 1 - \\alpha\n",
    "$$\n",
    "$$\n",
    "\\mathcal{P}(\\varsigma_j | s_i, a_w) = \\alpha p_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(s_i | \\varsigma_i, a_w) = 1 - \\alpha\n",
    "$$\n",
    "$$\n",
    "\\mathcal{P}(\\varsigma_j | \\varsigma_i, a_w) = \\alpha p_j\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{P}(\\varsigma_j | \\varsigma_i, a_r) = p_j\n",
    "$$\n",
    "\n",
    "in which all unspecified probabilities are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward function can be defined as\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(s_i, a_w) = w_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(\\varsigma_i, a_w) = w_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{R}(\\varsigma_i, a_r) = w_0\n",
    "$$\n",
    "\n",
    "in which all unspecified rewards are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write Bellman's Optimality Equation for this MDP, we start with the definition for some state $s \\in \\mathcal{S}$,\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_{a \\in \\mathcal{A}} \\{  \\mathcal{R}(s, a) + \\gamma \\sum_{s' \\in \\mathcal{N}} \\mathcal{P}(s, a, s') V^*(s') \\}\n",
    "$$\n",
    "\n",
    "For $s \\in \\{s_1, s_2, \\dots, s_n\\}$, the only action is $a_w$, so\n",
    "\n",
    "$$\n",
    "V^*(s_i) = \\max_{a \\in \\mathcal{A}} \\{  \\log(w_i) + \\gamma (1 - \\alpha) V^*(s_i) + \\gamma \\alpha \\sum_{j = 1}^{n} p_j V^*(\\varsigma_j) \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^*(s_i) =  \\log(w_i) + \\gamma (1 - \\alpha) V^*(s_i) + \\gamma \\alpha \\sum_{j = 1}^{n} p_j V^*(\\varsigma_j)\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^*(s_i) =  \\frac{\\log(w_i) + \\gamma \\alpha \\sum_{j = 1}^{n} p_j V^*(\\varsigma_j)}{1 - \\gamma (1 - \\alpha)}\n",
    "$$\n",
    "\n",
    "Now for $s \\in \\{\\varsigma_1, \\varsigma_2, \\dots, \\varsigma_n\\}$, the two actions are available, but note that $a_w$ reduces problem to the same as above,\n",
    "\n",
    "$$\n",
    "V^*(\\varsigma_i) = \\max_{a \\in \\mathcal{A}} \\{  V^*(s_i), \\log(w_0) + \\gamma \\sum_{j = 1}^{n} p_j V^*(\\varsigma_j) \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^*(\\varsigma_i) = \\max \\left \\{  \\frac{\\log(w_i) + \\gamma \\alpha \\sum_{j = 1}^{n} p_j V^*(\\varsigma_j)}{1 - \\gamma (1 - \\alpha)}, \\log(w_0) + \\gamma \\sum_{j = 1}^{n} p_j V^*(\\varsigma_j) \\right \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided above a couple different formulations for Bellman's Optimality Equation for both states in which you start employed and unemployed.\n",
    "\n",
    "Interestingly, the optimal values (and optimal actions) for the unemployed states $\\{\\varsigma_i\\}_{i=1}^n$ do not depend on those of the employed states $\\{s_i\\}_{i=1}^n$. This finding makes sense since there is a unique action available on the employed states, so there is no policy to be optimized for those states.\n",
    "<!-- Therefore, in our implementation of a DP numerical algorithm, we will omit the employed states and use the Bellman's Optimality Equation as defined in the last equation above. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_policy(pi):\n",
    "    assert(len(pi) % 2 == 0)\n",
    "    n = len(pi) // 2\n",
    "    assert(pi[:n].all())\n",
    "    return (n*[\"Work\"]) + [\"Accept\" if accept else \"Reject\" for accept in pi[n:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Q3():\n",
    "    alpha: float\n",
    "    gamma: float\n",
    "    unemp_wage: float\n",
    "    wages: np.array\n",
    "    probabilities: np.array\n",
    "\n",
    "    pi: Optional[np.array] = None\n",
    "    V: Optional[np.array] = None\n",
    "    eps: Optional[float] = 1e-10\n",
    "\n",
    "    n: int = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert(len(self.wages.shape) == 1 and len(self.probabilities.shape) == 1)\n",
    "        assert(self.wages.shape == self.probabilities.shape)\n",
    "        assert(np.abs(1 - self.probabilities.sum()) < self.eps)\n",
    "\n",
    "        self.n = len(self.wages)\n",
    "        if self.pi is None:\n",
    "            self.pi = np.ones(2*self.n).astype(bool)\n",
    "        assert(self.pi[:self.n].all())\n",
    "\n",
    "        if self.V is None:\n",
    "            self.V = np.zeros(2*self.n)\n",
    "        self.V = self.value_iteration()\n",
    "\n",
    "    # Reward matrix R\n",
    "    def R(self, pi=None) -> np.array:\n",
    "        if pi is None:\n",
    "            pi = self.pi\n",
    "        return np.array([self.get_reward(i, action) for (i, action) in enumerate(pi)])\n",
    "\n",
    "    # Transition probability matrix P\n",
    "    def P(self, pi=None) -> np.array:\n",
    "        if pi is None:\n",
    "            pi = self.pi\n",
    "        # employed -> employed\n",
    "        top_left = (1 - self.alpha) * np.eye(self.n)\n",
    "        # employed -> unemployed\n",
    "        top_right = np.repeat([self.alpha * self.probabilities], self.n, axis=0)\n",
    "        # unemployed -> employed\n",
    "        bottom_left = np.diag([(1 - self.alpha) * int(accept) for accept in pi[self.n:]])\n",
    "        # unmployed -> unemployed\n",
    "        bottom_right = [row if accept else row/self.alpha for (row, accept) in zip(top_right, pi[self.n:])]\n",
    "\n",
    "        top = np.concatenate([top_left, top_right], axis=1)\n",
    "        bottom = np.concatenate([bottom_left, bottom_right], axis=1)\n",
    "        P = np.concatenate([top, bottom], axis=0)\n",
    "\n",
    "        assert(P.shape == (2*self.n, 2*self.n))\n",
    "        for row in P:\n",
    "            assert(np.abs(1 - np.sum(row)) < self.eps)\n",
    "        return P\n",
    "\n",
    "    def get_reward(self, state: int, accept: bool) -> float:\n",
    "        # UNEMPLOYED AND REJECTED\n",
    "        if state >= self.n and not accept:\n",
    "            return np.log(self.unemp_wage)\n",
    "        else:\n",
    "            return np.log(self.wages[state % self.n])\n",
    "\n",
    "    def get_updated_V(self, V=None, pi=None) -> np.array:\n",
    "        if V is None:\n",
    "            V = self.V\n",
    "        return self.R(pi=pi) + self.gamma * np.matmul(self.P(pi=pi), V)\n",
    "\n",
    "    # Iterative algo to compute value vector from Bellman's operator\n",
    "    def value_iteration(self, pi=None) -> np.array:\n",
    "        V = self.V.copy()\n",
    "        while np.linalg.norm(self.get_updated_V(V, pi=pi) - V, ord=np.inf) > self.eps:\n",
    "            V = self.get_updated_V(V, pi=pi)\n",
    "        return V\n",
    "\n",
    "    # Greedy algo to get better policy\n",
    "    def greedy_iteration(self) -> None:\n",
    "        new_pi = self.pi.copy()\n",
    "        for i in range(self.n, 2*self.n):\n",
    "            test_pi = self.pi.copy()\n",
    "            test_pi[i] = not self.pi[i]\n",
    "            if self.value_iteration(pi=test_pi)[i] > self.V[i]:\n",
    "                new_pi[i] = test_pi[i]\n",
    "        self.pi = new_pi\n",
    "\n",
    "    # Run value iteration and greedy iteration in a loop until convergence\n",
    "    def policy_iteration(self) -> None:\n",
    "        old_pi = None\n",
    "        while not np.array_equal(old_pi, self.pi):\n",
    "            old_pi = self.pi.copy()\n",
    "            self.V = self.value_iteration()\n",
    "            self.greedy_iteration()\n",
    "        print(f\"Optimal value:\\n{self.V}\")\n",
    "        print(f\"\\nOptimal policy:\\n{format_policy(self.pi)}\")\n",
    "        print(f\"\\nFinal R vector:\\n{self.R()}\")\n",
    "        print(f\"\\nFinal P matrix:\\n{self.P()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unemployment wage: 1.0\n",
      "\n",
      "Wages:\n",
      "[1.21615405 0.65993361 1.6378637  0.77585731 1.86649041]\n",
      "\n",
      "Initial value:\n",
      "[ 1.02996559 -2.18745283  2.59680405 -1.33571927  3.2845257   0.\n",
      "  0.          0.          0.          0.        ]\n",
      "\n",
      "Initial policy:\n",
      "['Work', 'Work', 'Work', 'Work', 'Work', 'Reject', 'Reject', 'Reject', 'Reject', 'Reject']\n",
      "\n",
      "Initial R vector:\n",
      "[ 0.19569346 -0.41561604  0.49339277 -0.25378666  0.62405988  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "\n",
      "Initial P matrix:\n",
      "[[0.9  0.   0.   0.   0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.9  0.   0.   0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.9  0.   0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.   0.9  0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.   0.   0.9  0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]]\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "unemp_wage = 1.0\n",
    "wages = 2*np.random.random(n)\n",
    "probabilities = np.ones(n)/n\n",
    "\n",
    "start_pi = np.append(np.ones(n), np.zeros(n)).astype(bool)\n",
    "start_V = np.zeros(2*n)\n",
    "\n",
    "q3 = Q3(alpha, gamma, unemp_wage, wages, probabilities, start_pi, start_V)\n",
    "\n",
    "print(f\"Unemployment wage: {q3.unemp_wage}\")\n",
    "print(f\"\\nWages:\\n{q3.wages}\")\n",
    "print(f\"\\nInitial value:\\n{q3.V}\")\n",
    "print(f\"\\nInitial policy:\\n{format_policy(q3.pi)}\")\n",
    "print(f\"\\nInitial R vector:\\n{q3.R()}\")\n",
    "print(f\"\\nInitial P matrix:\\n{q3.P()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value:\n",
      "[ 3.08957523 -0.12784319  4.65641368  0.72389037  5.34413534  3.91325831\n",
      "  3.91325831  4.65641368  3.91325831  5.34413534]\n",
      "\n",
      "Optimal policy:\n",
      "['Work', 'Work', 'Work', 'Work', 'Work', 'Reject', 'Reject', 'Accept', 'Reject', 'Accept']\n",
      "\n",
      "Final R vector:\n",
      "[ 0.19569346 -0.41561604  0.49339277 -0.25378666  0.62405988  0.\n",
      "  0.          0.49339277  0.          0.62405988]\n",
      "\n",
      "Final P matrix:\n",
      "[[0.9  0.   0.   0.   0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.9  0.   0.   0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.9  0.   0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.   0.9  0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.   0.   0.9  0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]\n",
      " [0.   0.   0.9  0.   0.   0.02 0.02 0.02 0.02 0.02]\n",
      " [0.   0.   0.   0.   0.   0.2  0.2  0.2  0.2  0.2 ]\n",
      " [0.   0.   0.   0.   0.9  0.02 0.02 0.02 0.02 0.02]]\n"
     ]
    }
   ],
   "source": [
    "q3.policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "**Two-Stores Inventory Control.** \n",
    "We extend the capacity-constrained inventory example implemented in [rl/chapter3/simple_inventory_mdp_cap.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/chapter3/simple_inventory_mdp_cap.py) as a `FiniteMarkovDecisionProcess` (the Finite MDP model for the capacity-constrained inventory example is described in detail in Chapters 1 and 2 of the RLForFinanceBook). Here we assume that we have two different stores, each with their own separate capacities $C_1$ and $C_2$, their own separate Poisson probability distributions of demand (with means $\\lambda_1$ and $\\lambda_2$), their own separate holding costs $h_1$ and $h_2$, and their own separate stockout costs $p_1$ and $p_2$. At 6pm upon stores closing each evening, each store can choose to order inventory from a common supplier (as usual, ordered inventory will arrive at the store 36 hours later). We are also allowed to transfer inventory from one store to another, and any such transfer happens overnight, i.e., will arrive by 6am next morning (since the stores are fairly close to each other). Note that the orders are constrained such that following the orders on each evening, each store's inventory position (sum of on-hand inventory and on-order inventory) cannot exceed the store's capacity (this means the action space is constrained to be finite). Each order made to the supplier incurs a fixed transportation cost of $K_1$ (fixed-cost means the cost is the same no matter how many units of non-zero inventory a particular store orders). Moving any non-zero inventory between the two stores incurs a fixed transportation cost of $K_2$. \n",
    "\n",
    "Model this as a derived class of `FiniteMarkovDecisionProcess` much like we did for `SimpleInventoryMDPCap` in the code repo. Set up instances of this derived class for different choices of the problem parameters (capacities, costs etc.), and determine the Optimal Value Function and Optimal Policy by invoking the function `value_iteration` (or `policy_iteration`) from file [rl/dynamic_programming.py](https://github.com/TikhonJelvis/RL-book/blob/master/rl/dynamic_programming.py).\n",
    "\n",
    "Analyze the obtained Optimal Policy and verify that it makes intuitive sense as a function of the problem parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
